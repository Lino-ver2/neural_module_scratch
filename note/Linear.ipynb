{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"\n",
    "        in_dim : explanatory_variable_dim_\n",
    "        out_dim : target_variable_dim_\n",
    "        dw : parameter_gradient_\n",
    "        db : bias_gradient_\n",
    "        \"\"\"\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.weight = np.random.randn(out_dim, in_dim)\n",
    "        self.bias = np.zeros(out_dim, dtype=float)\n",
    "\n",
    "        self.dx , self.dw, self.db = None, None, None\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" forward_propagation\n",
    "        x : input_data_ (batch_size, in_dim)\n",
    "        output : output_data_ (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        # Affine\n",
    "        output = np.dot(self.x, self.weight.T) + self.bias\n",
    "        self.param = {'w' : self.weight, 'b' : self.bias}\n",
    "        return  output\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" back_propagation\n",
    "        grad : previous_gradient_ (batch_size, out_dim)\n",
    "        dx : gradient_ (batch_size, in_dim)\n",
    "        \"\"\"\n",
    "        # transpose x_shape\n",
    "        if self.x.ndim == 2:\n",
    "            x_T = self.x.T\n",
    "        if self.x.ndim == 3:\n",
    "            x_T = np.transpose(self.x, (0, 2, 1))\n",
    "        if self.x.ndim == 4:\n",
    "            x_T = np.transpose(self.x, (0, 1, 3, 2))\n",
    "        # calculate gradient\n",
    "        dx = np.dot(grad, self.weight)\n",
    "        dw = np.dot(x_T, grad)\n",
    "        db = np.sum(grad, axis=0)\n",
    "        self.grad_param = {'w' : dw, 'b' : db}\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  Affine_output  ------------\n",
      " [[[ 0.37592799  1.12673461  1.38469528  0.62574414 -0.47870263]\n",
      "  [ 0.35669337  0.5067835   0.32429235  0.29012487  0.16871195]\n",
      "  [ 1.19829281 -2.3424583   4.82897799 -1.04301817 -1.91963209]]\n",
      "\n",
      " [[ 1.80976351  0.40517077  3.79630358  0.30860244 -1.49672565]\n",
      "  [ 1.18746135 -0.42686333 -0.7142571  -0.30638402 -0.07519201]\n",
      "  [ 0.02754747  2.91399935 -0.9284874   1.41278849 -0.07131813]]]\n",
      "\n",
      "----------------  params  ----------------\n",
      " {'w': array([[-0.65183611,  0.04739867, -0.86041337, -0.38455554],\n",
      "       [ 1.00629281, -0.57689187,  0.83569211, -1.12970685],\n",
      "       [ 0.52980418,  1.44156862, -2.4716445 , -0.79689526],\n",
      "       [ 0.57707213, -0.20304539,  0.37114587, -0.60398519],\n",
      "       [ 0.08658979, -0.15567724,  1.16778206,  0.25442084]]), 'b': array([0., 0., 0., 0., 0.])}\n",
      "----------------  grad  -------------\n",
      " [[[-1.108328   -0.42288572  0.87060102  1.08565993]\n",
      "  [ 2.14003873  2.67858639 -4.58267983 -2.42843937]\n",
      "  [-3.0171748  -2.14981808  1.10412909  1.75614459]]\n",
      "\n",
      " [[ 1.30553576  2.35714061 -3.96024199 -1.47477888]\n",
      "  [-2.32404214  1.72821486 -4.58437847  1.14944776]\n",
      "  [-0.06315537 -1.22170805  3.21175037 -0.29577698]]]\n",
      "\n",
      "---------------params_grad-----------\n",
      " {'w': array([[[[ 0.45343807, -0.31906383, -1.53135972, -0.71956361,\n",
      "           0.10889323],\n",
      "         [-0.45983486,  1.31392446,  0.27625669, -0.29333752,\n",
      "           1.01910208]],\n",
      "\n",
      "        [[ 0.56760817, -0.06703728,  0.06081544, -0.07769221,\n",
      "          -1.67178081],\n",
      "         [ 1.50358713, -1.41685567, -0.21828186,  1.04134474,\n",
      "           0.60026549]],\n",
      "\n",
      "        [[-2.29686072,  0.92481359,  4.55857235,  2.22262488,\n",
      "           2.86554555],\n",
      "         [-1.40993207, -1.67708441,  0.4107258 , -0.74834842,\n",
      "          -4.47900227]],\n",
      "\n",
      "        [[ 0.75783151, -0.13119828, -2.69372098, -1.0067524 ,\n",
      "          -1.05579828],\n",
      "         [ 0.12970151,  2.32871654, -1.97745075, -0.70644224,\n",
      "           2.87805153]]],\n",
      "\n",
      "\n",
      "       [[[ 1.22855272, -0.45929527, -4.35623436, -1.77492541,\n",
      "          -0.92542984],\n",
      "         [-0.38657889,  3.80267335, -1.75951962, -1.06715593,\n",
      "           4.00169041]],\n",
      "\n",
      "        [[-1.82344842,  0.45694278,  3.20795288,  1.47002649,\n",
      "           3.31775535],\n",
      "         [-2.06106149, -0.65823139,  1.65791161, -0.82776843,\n",
      "          -3.96891795]],\n",
      "\n",
      "        [[-0.44639344,  0.41659546,  1.04587706,  0.62301883,\n",
      "          -0.2621233 ],\n",
      "         [ 0.40813214, -0.61070737, -1.19091958, -0.09113337,\n",
      "          -0.36683856]],\n",
      "\n",
      "        [[-1.68331926,  0.97552703,  1.47145413,  1.21375283,\n",
      "           1.84844821],\n",
      "         [-1.50070954,  1.55639878, -2.73592147, -2.05348067,\n",
      "          -1.01728509]]]]), 'b': array([[-0.01391082, -0.39345859,  1.18883109, -0.10563081,  0.17535161],\n",
      "       [ 0.4084211 , -1.92136045,  2.31101536,  1.6419899 , -1.80460978],\n",
      "       [ 2.34496797, -0.06957897, -2.45341245, -0.37530444,  0.39990094]])}\n"
     ]
    }
   ],
   "source": [
    "batch, channel, indim, outdim = np.arange(2,6)\n",
    "# create_data\n",
    "x = np.random.randn(batch, channel, indim)\n",
    "\n",
    "# forward_propagation\n",
    "affine = Linear(indim, outdim)\n",
    "out = affine(x)\n",
    "print('-------------  Affine_output  ------------\\n',out)\n",
    "print()\n",
    "# parameters\n",
    "print('----------------  params  ----------------\\n', affine.param)\n",
    "# demo_grad\n",
    "grad = np.random.randn(batch, channel, outdim)\n",
    "#back_propagation\n",
    "dx = affine.backward(grad)\n",
    "print('----------------  grad  -------------\\n', dx)\n",
    "print()\n",
    "# grad_parameters\n",
    "print('---------------params_grad-----------\\n',affine.grad_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38795564\n",
      "0.38795564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True, False,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True, False]]],\n",
       "\n",
       "\n",
       "       [[[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Check my module against pytorch module ######\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# self_made module\n",
    "x = np.random.randn(2,3,3,4)\n",
    "affine = Linear(4,6)\n",
    "# torch.nn.Linear\n",
    "xt = torch.tensor(x).float()\n",
    "linear = nn.Linear(4,6)\n",
    "# overwrite nn.Linear params with self_made module params\n",
    "weight = linear.weight.detach().numpy().copy()\n",
    "affine.weight = weight\n",
    "bias = linear.bias.detach().numpy().copy()\n",
    "affine.bias = bias\n",
    "# output with self_made module\n",
    "y = affine(x).astype(dtype='float32')\n",
    "# output with nn.Linear\n",
    "yt = linear(xt).detach().numpy().copy()\n",
    "print(y[0][0][0][0])\n",
    "print(yt[0][0][0][0])\n",
    "np.round(y, decimals=6) ==\\\n",
    "np.round(yt, decimals=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8dc0e1aaedc0f4508af2e31239646ff268de5d64b0b4927e542f306107c7d4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
