{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"\n",
    "        in_dim : explanatory_variable_dim_\n",
    "        out_dim : target_variable_dim_\n",
    "        dw : parameter_gradient_\n",
    "        db : bias_gradient_\n",
    "        \"\"\"\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.weight = np.random.randn(out_dim, in_dim)\n",
    "        self.bias = np.zeros(out_dim, dtype=float)\n",
    "\n",
    "        self.dx , self.dw, self.db = None, None, None\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" forward_propagation\n",
    "        x : input_data_ (batch_size, in_dim)\n",
    "        output : output_data_ (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        # Affine\n",
    "        output = np.dot(self.x, self.weight.T) + self.bias\n",
    "        self.param = {'w' : self.weight, 'b' : self.bias}\n",
    "        return  output\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" back_propagation\n",
    "        grad : previous_gradient_ (batch_size, out_dim)\n",
    "        dx : gradient_ (batch_size, in_dim)\n",
    "        \"\"\"\n",
    "        # transpose x_shape\n",
    "        if self.x.ndim == 2:\n",
    "            x_T = self.x.T\n",
    "        if self.x.ndim == 3:\n",
    "            x_T = np.transpose(self.x, (0, 2, 1))\n",
    "        if self.x.ndim == 4:\n",
    "            x_T = np.transpose(self.x, (0, 1, 3, 2))\n",
    "        # calculate gradient\n",
    "        dx = np.dot(grad, self.weight)\n",
    "        dw = np.dot(x_T, grad)\n",
    "        db = np.sum(self.bias)\n",
    "        self.grad_param = {'w' : dw, 'b' : db}\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  Affine_output  ------------\n",
      " [[[ 1.60390525 -1.35064836 -2.04226026 -0.35020045 -0.54220778]\n",
      "  [ 0.6389963  -1.7398243  -0.95549327  0.29750484 -0.52366306]\n",
      "  [ 1.768501    0.06397159 -1.39201434 -2.27508346 -0.03210308]]\n",
      "\n",
      " [[ 0.01949151 -3.12780704  1.65338964 -2.69720329 -0.30316664]\n",
      "  [-1.22042728 -2.35410276  3.58903065  0.44679222 -0.02189353]\n",
      "  [-2.5205208   2.92718227 -0.75422601  0.71272238  0.3994948 ]]]\n",
      "\n",
      "----------------  params  ----------------\n",
      " {'w': array([[ 0.34175598,  1.87617084,  0.95042384, -0.57690366],\n",
      "       [-0.89841467,  0.49191917, -1.32023321,  1.83145877],\n",
      "       [ 1.17944012, -0.46917565, -1.71313453,  1.35387237],\n",
      "       [-0.11453985,  1.23781631, -1.59442766, -0.59937502],\n",
      "       [ 0.0052437 ,  0.04698059, -0.45006547,  0.62284993]]), 'b': array([0., 0., 0., 0., 0.])}\n",
      "----------------  grad  -------------\n",
      " [[[-0.1502606  -1.45930443 -2.17330917  0.6529044 ]\n",
      "  [ 2.45887279 -3.08795636 -1.42531317 -1.09642087]\n",
      "  [ 1.82017447  2.59422179 -1.73388523  2.15667377]]\n",
      "\n",
      " [[ 1.90273646  3.57816763 -1.3124023   0.3123828 ]\n",
      "  [ 0.49746235 -4.202053   -0.29933103  3.56326654]\n",
      "  [ 0.21972618 -2.43200461 -5.24044858  0.80016826]]]\n",
      "\n",
      "---------------params_grad-----------\n",
      " {'w': array([[[[-0.09789028, -0.32546303,  0.30213918,  0.02017352,\n",
      "          -0.19461625],\n",
      "         [-0.45712516,  0.17421314,  0.14910385, -0.42748845,\n",
      "          -0.41219583]],\n",
      "\n",
      "        [[ 0.32943345, -0.01890792,  0.65170117,  0.24446483,\n",
      "           1.30268774],\n",
      "         [-0.32254385, -0.22553693,  0.76095436,  0.98527958,\n",
      "           0.57179435]],\n",
      "\n",
      "        [[ 1.4042818 , -0.08024813,  1.91914936,  0.48547502,\n",
      "           3.40844867],\n",
      "         [-1.84203076, -0.49575829,  1.81516349,  2.45375136,\n",
      "           0.93649894]],\n",
      "\n",
      "        [[ 2.78568535,  1.35397413,  0.09534363, -0.39851686,\n",
      "           2.26988832],\n",
      "         [-1.91455934, -0.68886909, -0.51966929,  2.54175208,\n",
      "           0.43662437]]],\n",
      "\n",
      "\n",
      "       [[[-8.05601118, -3.91165758,  0.55217465,  1.69198066,\n",
      "          -4.48361505],\n",
      "         [ 5.99769339,  1.53844429,  2.88572564, -5.65253043,\n",
      "           0.19844091]],\n",
      "\n",
      "        [[ 0.91275627,  1.28845988, -1.74507396, -0.69886041,\n",
      "          -1.02450413],\n",
      "         [ 0.50512602, -0.22185019, -1.97830674,  0.13648668,\n",
      "          -0.13274597]],\n",
      "\n",
      "        [[-1.16445093,  1.07621622, -1.86319978,  0.11844879,\n",
      "          -0.31510422],\n",
      "         [ 3.8895351 , -0.58875343, -0.58597053,  0.89933841,\n",
      "           2.1312458 ]],\n",
      "\n",
      "        [[ 0.71164615,  0.91082936, -0.56641588, -0.09471397,\n",
      "           0.88902991],\n",
      "         [ 0.59350789, -0.4973769 , -0.34836819,  1.3984452 ,\n",
      "           0.9713075 ]]]]), 'b': 0.0}\n"
     ]
    }
   ],
   "source": [
    "batch, channel, indim, outdim = np.arange(2,6)\n",
    "# create_data\n",
    "x = np.random.randn(batch, channel, indim)\n",
    "\n",
    "# forward_propagation\n",
    "affine = Linear(indim, outdim)\n",
    "out = affine(x)\n",
    "print('-------------  Affine_output  ------------\\n',out)\n",
    "print()\n",
    "# parameters\n",
    "print('----------------  params  ----------------\\n', affine.param)\n",
    "# demo_grad\n",
    "grad = np.random.randn(batch, channel, outdim)\n",
    "#back_propagation\n",
    "dx = affine.backward(grad)\n",
    "print('----------------  grad  -------------\\n', dx)\n",
    "print()\n",
    "# grad_parameters\n",
    "print('---------------params_grad-----------\\n',affine.grad_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.60770345\n",
      "-0.60770345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ True,  True, False,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True, False]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "       [[[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True, False,  True],\n",
       "         [ True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Check my module against pytorch module ######\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# self_made module\n",
    "x = np.random.randn(2,3,3,4)\n",
    "affine = Linear(4,6)\n",
    "# torch.nn.Linear\n",
    "xt = torch.tensor(x).float()\n",
    "linear = nn.Linear(4,6)\n",
    "# overwrite nn.Linear params with self_made module params\n",
    "weight = linear.weight.detach().numpy().copy()\n",
    "affine.weight = weight\n",
    "bias = linear.bias.detach().numpy().copy()\n",
    "affine.bias = bias\n",
    "# output with self_made module\n",
    "y = affine(x).astype(dtype='float32')\n",
    "# output with nn.Linear\n",
    "yt = linear(xt).detach().numpy().copy()\n",
    "print(y[0][0][0][0])\n",
    "print(yt[0][0][0][0])\n",
    "np.round(y, decimals=6) ==\\\n",
    "np.round(yt, decimals=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8dc0e1aaedc0f4508af2e31239646ff268de5d64b0b4927e542f306107c7d4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
