{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"\n",
    "        in_dim : explanatory_variable_dim_\n",
    "        out_dim : target_variable_dim_\n",
    "        dw : parameter_gradient_\n",
    "        db : bias_gradient_\n",
    "        \"\"\"\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.weight = np.random.randn(out_dim, in_dim)\n",
    "        self.bias = np.zeros(out_dim, dtype=float)\n",
    "\n",
    "        self.dx , self.dw, self.db = None, None, None\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" forward_propagation\n",
    "        x : input_data_ (batch_size, in_dim)\n",
    "        output : output_data_ (batch_size, out_dim)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        # Affine\n",
    "        output = np.dot(self.x, self.weight.T) + self.bias\n",
    "        self.param = {'w' : self.weight, 'b' : self.bias}\n",
    "        return  output\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" back_propagation\n",
    "        grad : previous_gradient_ (batch_size, out_dim)\n",
    "        dx : gradient_ (batch_size, in_dim)\n",
    "        \"\"\"\n",
    "        # transpose x_shape\n",
    "        if self.x.ndim == 2:\n",
    "            x_T = self.x.T\n",
    "        if self.x.ndim == 3:\n",
    "            x_T = np.transpose(self.x, (0, 2, 1))\n",
    "        if self.x.ndim == 4:\n",
    "            x_T = np.transpose(self.x, (0, 1, 3, 2))\n",
    "        # calculate gradient\n",
    "        dx = np.dot(grad, self.weight)\n",
    "        dw = np.dot(x_T, grad)\n",
    "        db = np.sum(self.bias)\n",
    "        self.grad_param = {'w' : dw, 'b' : db}\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  Affine_output  ------------\n",
      " [[[-0.45902529  2.1733416  -1.18708902 -0.18572742 -0.1903813 ]\n",
      "  [-1.42784213  0.68021768  0.61286529 -1.5364852  -0.62674877]\n",
      "  [ 0.67417295 -0.46009647 -0.38013374 -0.63747404 -0.05988715]]\n",
      "\n",
      " [[ 1.43017296  0.40914153  1.28807438  5.98025443  0.21959614]\n",
      "  [ 1.10075507 -1.55293367  0.65859899  0.1006324  -0.16382862]\n",
      "  [-1.43591716 -3.49412997  2.01396363  0.37840003  1.46482916]]]\n",
      "\n",
      "----------------  params  ----------------\n",
      " {'w': array([[-0.54438272,  0.11092259, -1.15099358,  0.37569802],\n",
      "       [-0.60063869, -0.29169375, -0.60170661,  1.85227818],\n",
      "       [-0.01349722, -1.05771093,  0.82254491, -1.22084365],\n",
      "       [ 0.2088636 , -1.95967012, -1.32818605,  0.19686124],\n",
      "       [ 0.73846658,  0.17136828, -0.11564828, -0.3011037 ]]), 'b': array([0., 0., 0., 0., 0.])}\n",
      "----------------  grad  -------------\n",
      " [[[ 1.71801043 -1.47953034  0.3122144  -1.22181994]\n",
      "  [ 1.08062691  1.54857722  2.345836    0.09061054]\n",
      "  [-0.92925033  1.39308439 -2.06494382  2.97627114]]\n",
      "\n",
      " [[-1.35518516  2.40741537  0.6205212  -0.15200527]\n",
      "  [-0.77930571 -2.13393387 -3.18520313  2.99397804]\n",
      "  [ 1.26131894 -1.46543577  1.65681402 -1.92883994]]]\n",
      "\n",
      "---------------params_grad-----------\n",
      " {'w': array([[[[-0.80560886, -0.87065432,  0.25535365,  0.82875843,\n",
      "          -0.1280657 ],\n",
      "         [ 0.59717861, -0.72524679, -0.7974268 , -0.76769345,\n",
      "          -1.41341118]],\n",
      "\n",
      "        [[ 1.17659881,  0.5289241 , -0.301474  , -0.15543609,\n",
      "          -0.01099587],\n",
      "         [-0.67514159, -0.05521441,  0.87700743, -0.10143528,\n",
      "           0.92963344]],\n",
      "\n",
      "        [[-4.21962077, -0.38600897, -0.51756908, -0.24102346,\n",
      "           1.03501133],\n",
      "         [ 2.2139605 ,  1.66395885, -0.94672186,  0.88483395,\n",
      "          -0.92875263]],\n",
      "\n",
      "        [[-4.08501853, -1.28135579, -0.60624355,  1.23454466,\n",
      "           0.83848331],\n",
      "         [ 2.40979635,  0.14271634, -1.05433662, -0.89814995,\n",
      "          -2.27302746]]],\n",
      "\n",
      "\n",
      "       [[[ 2.93899858,  0.86251209, -0.95143202,  0.48819463,\n",
      "          -0.05086628],\n",
      "         [-1.5324253 , -0.95990517,  2.28223302, -1.33660906,\n",
      "           1.63750496]],\n",
      "\n",
      "        [[ 2.04202099,  1.26884525,  0.94979541, -2.16547918,\n",
      "          -0.54001187],\n",
      "         [-1.4655044 ,  1.26139485, -0.01466354,  2.44017506,\n",
      "           2.0490622 ]],\n",
      "\n",
      "        [[ 4.2208334 ,  1.01028374,  1.08755716, -1.22966806,\n",
      "          -1.12576049],\n",
      "         [-2.46409197, -0.38055801,  0.48949425,  0.99468563,\n",
      "           1.8406415 ]],\n",
      "\n",
      "        [[ 1.85238791, -1.37978523,  1.99853821,  0.80216257,\n",
      "          -1.52905194],\n",
      "         [-0.77693689, -2.16088685, -1.98522092, -0.74417178,\n",
      "          -2.06735104]]]]), 'b': 0.0}\n"
     ]
    }
   ],
   "source": [
    "batch, channel, indim, outdim = np.arange(2,6)\n",
    "# create_data\n",
    "x = np.random.randn(batch, channel, indim)\n",
    "\n",
    "# forward_propagation\n",
    "affine = Linear(indim, outdim)\n",
    "out = affine(x)\n",
    "print('-------------  Affine_output  ------------\\n',out)\n",
    "print()\n",
    "# parameters\n",
    "print('----------------  params  ----------------\\n', affine.param)\n",
    "# demo_grad\n",
    "grad = np.random.randn(batch, channel, outdim)\n",
    "#back_propagation\n",
    "dx = affine.backward(grad)\n",
    "print('----------------  grad  -------------\\n', dx)\n",
    "print()\n",
    "# grad_parameters\n",
    "print('---------------params_grad-----------\\n',affine.grad_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2805776394511505\n",
      "0.28057763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lino/opt/anaconda3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# self_made module\n",
    "x = np.random.randn(2,3,3,4)\n",
    "affine = Linear(4,6)\n",
    "# torch.nn.Linear\n",
    "xt = torch.tensor(x).float()\n",
    "linear = nn.Linear(4,6)\n",
    "# overwrite nn.Linear params with self_made module params\n",
    "weight = linear.weight.detach().numpy().copy()\n",
    "affine.weight = weight\n",
    "bias = linear.bias.detach().numpy().copy()\n",
    "affine.bias = bias\n",
    "# output with self_made module\n",
    "y = affine(x)\n",
    "# output with nn.Linear\n",
    "yt = linear(xt).detach().numpy().copy()\n",
    "print(y[0][0][0][0])\n",
    "print(yt[0][0][0][0])\n",
    "np.round(y[0][0], decimals=3) == np.round(yt[0][0], decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8dc0e1aaedc0f4508af2e31239646ff268de5d64b0b4927e542f306107c7d4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
